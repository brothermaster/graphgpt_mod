{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "task_specific",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "./vicuna-7b-v1.5-16k",
                "--version",
                "v1",
                "--data_path",
                "./data/stage_2/data_all_mix.json",
                "--graph_content",
                "./arxiv_ti_ab.json",
                "--graph_data_path",
                "./graph_data/all_graph_data.pt",
                "--graph_tower",
                "clip_gt_arxiv",
                "--pretrain_graph_mlp_adapter",
                "./checkpoints/stage_1_projector/stage_1_projector.bin",
                "--tune_graph_mlp_adapter",
                "True",
                "--graph_select_layer",
                "-2",
                "--use_graph_start_end",
                "--bf16",
                "True",
                "--output_dir",
                "./checkpoints/stage_2",
                "--num_train_epochs",
                "2",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "2",
                "--gradient_accumulation_steps",
                "1",
                "--evaluation_strategy",
                "no",
                "--save_strategy",
                "steps",
                "--save_steps",
                "50000",
                "--save_total_limit",
                "1",
                "--learning_rate",
                "2e-5",
                "--weight_decay",
                "0.",
                "--warmup_ratio",
                "0.03",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "1",
                "--tf32",
                "True",
                "--model_max_length",
                "2048",
                "--gradient_checkpointing",
                "True",
                "--dataloader_num_workers",
                "4",
                "--lazy_preprocess",
                "True",
                "--report_to",
                "wandb"
            ]
        },
        {
            "name": "extract_projector",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "./checkpoints/stage_1",
                "--output",
                "./checkpoints/stage_1_projector/stage_1_projector.bin"
            ]
        },
        {
            "name": "mem",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "./vicuna-7b-v1.5-16k",
                "--version",
                "v1",
                "--data_path",
                "./data/stage_1/graph_matching.json",
                "--graph_content",
                "./arxiv_ti_ab.json",
                "--graph_data_path",
                "./graph_data/all_graph_data.pt",
                "--graph_tower",
                "clip_gt_arxiv",
                "--tune_graph_mlp_adapter",
                "True",
                "--graph_select_layer",
                "-2",
                "--use_graph_start_end",
                "--bf16",
                "True",
                "--output_dir",
                "./checkpoints/stage_1",
                "--num_train_epochs",
                "3",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "2",
                "--gradient_accumulation_steps",
                "1",
                "--evaluation_strategy",
                "no",
                "--save_strategy",
                "steps",
                "--save_steps",
                "2400",
                "--save_total_limit",
                "1",
                "--learning_rate",
                "2e-3",
                "--weight_decay",
                "0.",
                "--warmup_ratio",
                "0.03",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "1",
                "--tf32",
                "True",
                "--model_max_length",
                "2048",
                "--gradient_checkpointing",
                "True",
                "--lazy_preprocess",
                "True",
                "--report_to",
                "wandb"
            ]
        }
    ]
}